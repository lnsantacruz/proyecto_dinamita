#Proyecto Inteligencia Artificial


BIG DATA A TRAVES DEL TIEMPO 

¿Qué es big data?
Se trata del procesamiento de una cantidad de datos masivos. Tan ingente, que no son capaz de ser procesados y analizados por la tecnología y software convencional.  El análisis de esa gran cantidad de datos busca encontrar patrones repetitivos. De ellos obtiene información suficiente para poder tomar decisiones automáticamente en base a ellos.

1936. El concepto de algoritmo
El considerado padre de la computación moderna Alan Turing, publica su este año su artículo sobre los números computables en el que introduce el concepto de algoritmo
y sienta las bases de la informática.






![image](https://user-images.githubusercontent.com/67651082/222308090-9e1d17b8-2cda-495a-b8ad-6e4a7f0f1b7f.png)

1941. Konrad Zuse creo el denominado Z3
Este tiene un lugar muy destacado en la historia de la informática, ya que tiene el honor de ser el primer ordenador binario programable y completamente automático. Esta máquina construida por el ejército alemán con fines militares justo en medio de la Segunda Guerra Mundial.


1956. John McCarthy acuña por primera vez el término Inteligencia Artificial durante la conferencia de Darmouth“, y dijo: "Este estudio procederá sobre la base de que todos los aspectos del aprendizaje o de rasgo de la inteligencia pueden, en principio, ser descritos de una forma tan precisa que se puede crear una máquina que los simule”.

1965. El gobierno de los Estados Unidos decidió construir el primer centro de datos para almacenar más de 742 millones de declaraciones de impuestos. Así como 175 millones de conjuntos de huellas dactilares transfiriendo todos esos registros a cintas magnéticas de computadora que debían almacenarse en un solo lugar.

1989. El científico informático británico Tim Berners-Lee inventó finalmente la World Wide Web. Quería facilitar el intercambio de información a través de un sistema de «hipertexto». Pero él no sabía en ese momento el impacto que su invento iba a tener en el futuro.

A partir de los años 90, la creación de datos se ve impulsada a medida que cada vez más dispositivos están conectados a Internet. En 1995 se construyó la primera supercomputadora. Que fue capaz de hacer tanto trabajo en un segundo de lo que puede hacer una calculadora operada por una sola persona en 30.000 años.


![image](https://user-images.githubusercontent.com/94588467/222309194-40e4413f-0b63-4ae0-94e8-e564c9683276.png)


![image](https://user-images.githubusercontent.com/125932078/222309600-70a8c00d-f90b-4e3f-b1f1-6d80c536dff5.png)

En 2020 se espera que la generación anual de datos haya aumentado hasta un 4.300%. El crecimiento estará motivado por el cambio de las tecnologías analógicas a digitales y por la cantidad de información generada tanto por los usuarios como por las empresas.

Según Gartner, en 2020 habrá más de 25.000 millones de dispositivos conectados a Internet. A finales de 2013, la cantidad de datos generados por los dispositivos era de 4.4 billones de GB. Cifra que se espera se multiplique por 10 en 2020. Esta gran cantidad de datos requerirá de nuevas técnicas. También de una capacidad de gestión mayor y tener muy en cuenta las 3 Vs del Big Data: Velocidad, Variedad y Volumen.

Lo conseguido ya con el Big Data permite ver el futuro con optimismo. Será siempre y cuando la tecnología crezca al nivel que lo hacen los datos. Áreas como el medio ambiente, la salud, la productividad o la vida personal podrán verse beneficiadas por esos miles de millones de bytes que generamos diariamente.

Data Warehouse de nueva generación: La confluencia entre el cloud y el Big Data está al alza. Prueba de ello es el Data Warehouse de nueva generación, es decir, las tecnologías y prácticas que se piensan y crean por y para la nube. Así el Data Fabrics, Data Mesh y Data Vault son alguna de las tendencias que, en este sentido, predominarán en las conversaciones sobre el Big Data corporativo del futuro.
 
Automatización robótica de procesos (RPA): Es otro de los platos fuertes del último año y que seguirá siendo tendencia en 2022. Los robots software que se encargan de tareas y procesos rutinarios son y serán cada vez más importantes en las organizaciones para impulsar la eficiencia en los procesos internos. Pero esta integración se debe hacer de manera profesional, acompañado de auténticos líderes del mercado, para que la inversión sea fructífera. ¿Cómo hacerlo? ¿Cuándo? ¿Por qué procesos se podría empezar en función de la empresa? En Big Data & AI World Madrid se darán cita los líderes del sector que presentarán sus novedades y herramientas.
 
La realidad extendida: En 2022 daremos un paso al frente para introducirnos de lleno en el metaverso. Esa palabra que empezó a retumbar en 2021 será una de las indiscutibles tendencias del año. ¿Cómo está impactando en el ecosistema empresarial de IA? ¿Qué deben tener presente las empresas para integrarlas? ¿Qué servicios y herramientas destacarán entorno al metaverso? Son muchas las incógnitas que giran en torno a este universo donde confluye la Realidad Aumentada y la Realidad Virtual, pero grandes compañías como Meta ya apuestan todo al metaverso y otras como Inditex, Nike, BMW o Coca-Cola empiezan a adentrarse de lleno en este mundo virtual. Sin duda, la realidad extendida será una de las piezas claves de este 2022 y en Big Data & AI World Madrid se analizarán los retos, pero, sobre todo, las oportunidades que el metaverso ofrece a las empresas.
 
El impacto de la computación cuántica: Sin duda, la computación cuántica provocará un salto cualitativo sin precedentes. Se podrán procesar, en cuestión de segundos, datos que de otra forma las supercomputadoras procesarían durante años. Estas máquinas, que multinacionales como IBM quiere acercar a todo tipo de empresas, permitirán analizar la ingente cantidad de datos que se generan en un mundo hiperconectado con el objetivo de obtener resultados, líneas de actuación e información valiosa.
 
Gobierno del Dato: Es un tema trascendental que seguirá estando en boca de todos durante 2022. Saber cuál es la correcta gestión de los datos que gestiona una empresa tanto propios como ajenos es un reto mayúsculo en términos de operatividad, de resultados, de seguridad o de reputación. Es un activo fundamental para las empresas, no solo el saber cómo detectar el Smart Data dentro del lago de datos, sino, el cómo albergarlos, protegerlos y, sobre todo, explotarlos.

Las 7 V del Big Data
Volumen: Son los grandes volúmenes de datos generados que deben ser procesados.
Velocidad: El ritmo y constancia a la que se reciben esos datos es la velocidad.
Variedad: La información que se proporciona proviene de fuentes diferentes
Veracidad: Calidad y fiabilidad que se obtiene de una limpieza y eliminar aquellos datos que no sean correctos.
Valor: Hace referencia a la toma de decisiones útiles y que se apliquen a la informacion
Visualización: la visualización es la manera en la que se presentan los datos, con el objetivo de que esta representación se realice de forma sencilla para que cualquier persona pueda acceder a estos datos.
Viabilidad: Es la capacidad que se tiene para gestionar y manejar el gran número de datos.
#Referencias
1. https://www.nationalgeographic.com.es/ciencia/breve-historia-visual-inteligencia-artificial_14419
2. https://parceladigital.com/articulo/la-maquina-z3-de-zuse
3. https://www.bbvaopenmind.com/tecnologia/inteligencia-artificial/el-verdadero-padre-de-la-inteligencia-artificial/#:~:text=En%201956%2C%20John%20organiza%20la,ingenier%C3%ADa%20de%20hacer%20m%C3%A1quinas%20inteligentes.
4. https://www.egosbi.com/historia-del-big-data/#:~:text=La%20primera%20m%C3%A1quina%20de%20procesamiento,de%205.000%20caracteres%20por%20segundo.
5. https://www.bigdataworld.es/noticias/5-tendencias-big-data-ia-2022
